{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e12e5cde",
   "metadata": {},
   "source": [
    "# ðŸ§  CNN Architecture & Training Pipeline\n",
    "#\n",
    "## Multi-Task Deep Learning for Fire Pattern Classification\n",
    "#\n",
    "This notebook demonstrates the multi-task Convolutional Neural Network (CNN) architecture\n",
    "that learns to classify fire characteristics from our 4-channel fingerprints.\n",
    "#\n",
    "**Architecture**: Simultaneous prediction of fire type, ignition cause, state, and size category\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca5d6f79",
   "metadata": {},
   "source": [
    "## ðŸ“‹ What You'll Learn\n",
    "#\n",
    "1. **Multi-Task CNN Design**: Architecture for simultaneous classification\n",
    "2. **Transfer Learning**: Using pre-trained models (EfficientNet, ResNet)\n",
    "3. **Training Pipeline**: Complete training system with validation\n",
    "4. **Performance Evaluation**: Metrics and analysis for multi-task learning\n",
    "5. **Model Optimization**: Hyperparameter tuning and best practices\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcb4830c",
   "metadata": {},
   "source": [
    "## ðŸ› ï¸ Setup and Imports\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db094338",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models, optimizers, callbacks\n",
    "from tensorflow.keras.applications import EfficientNetB0, ResNet50V2\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "import json\n",
    "from pathlib import Path\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Import our data processing functions\n",
    "exec(open('02_Data_Processing_Pipeline.py').read())\n",
    "\n",
    "print(\"ðŸ”¥ Fire Fingerprinting System - CNN Architecture & Training\")\n",
    "print(\"=\" * 60)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55fec233",
   "metadata": {},
   "source": [
    "## ðŸ—ï¸ Multi-Task CNN Architecture Theory\n",
    "#\n",
    "### Why Multi-Task Learning?\n",
    "#\n",
    "Traditional single-task CNNs focus on one prediction target. Our multi-task architecture\n",
    "learns multiple related fire characteristics simultaneously, improving performance through:\n",
    "#\n",
    "1. **Shared Feature Learning**: Common features benefit all tasks\n",
    "2. **Regularization**: Joint learning prevents overfitting\n",
    "3. **Efficiency**: Single forward pass for multiple predictions\n",
    "4. **Correlations**: Learning relationships between fire characteristics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e76a1f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_custom_fire_cnn(input_shape=(224, 224, 4), num_classes_dict=None):\n",
    "    \"\"\"\n",
    "    Create a custom multi-task CNN for fire fingerprint classification\n",
    "\n",
    "    Architecture:\n",
    "    - Shared convolutional backbone\n",
    "    - Multiple output heads for different tasks\n",
    "    - Task-specific classification layers\n",
    "    \"\"\"\n",
    "    if num_classes_dict is None:\n",
    "        num_classes_dict = {\n",
    "            'fire_type': 3,\n",
    "            'ignition_cause': 11,\n",
    "            'state': 8,\n",
    "            'size_category': 4\n",
    "        }\n",
    "\n",
    "    print(f\"Creating custom multi-task CNN with {len(num_classes_dict)} tasks:\")\n",
    "    for task, classes in num_classes_dict.items():\n",
    "        print(f\"  {task}: {classes} classes\")\n",
    "\n",
    "    # Input layer\n",
    "    inputs = layers.Input(shape=input_shape, name='fire_fingerprint_input')\n",
    "\n",
    "    # Shared convolutional backbone\n",
    "    # Block 1\n",
    "    x = layers.Conv2D(32, (3, 3), activation='relu', padding='same')(inputs)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.Conv2D(32, (3, 3), activation='relu', padding='same')(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.MaxPooling2D((2, 2))(x)\n",
    "    x = layers.Dropout(0.25)(x)\n",
    "\n",
    "    # Block 2\n",
    "    x = layers.Conv2D(64, (3, 3), activation='relu', padding='same')(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.Conv2D(64, (3, 3), activation='relu', padding='same')(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.MaxPooling2D((2, 2))(x)\n",
    "    x = layers.Dropout(0.25)(x)\n",
    "\n",
    "    # Block 3\n",
    "    x = layers.Conv2D(128, (3, 3), activation='relu', padding='same')(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.Conv2D(128, (3, 3), activation='relu', padding='same')(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.MaxPooling2D((2, 2))(x)\n",
    "    x = layers.Dropout(0.25)(x)\n",
    "\n",
    "    # Block 4\n",
    "    x = layers.Conv2D(256, (3, 3), activation='relu', padding='same')(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.Conv2D(256, (3, 3), activation='relu', padding='same')(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.MaxPooling2D((2, 2))(x)\n",
    "    x = layers.Dropout(0.25)(x)\n",
    "\n",
    "    # Global pooling and feature extraction\n",
    "    x = layers.GlobalAveragePooling2D()(x)\n",
    "    x = layers.Dense(512, activation='relu')(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.Dropout(0.5)(x)\n",
    "\n",
    "    # Feature extraction point (for similarity search)\n",
    "    feature_layer = layers.Dense(256, activation='relu', name='feature_extraction')(x)\n",
    "\n",
    "    # Task-specific output heads\n",
    "    outputs = []\n",
    "    loss_weights = {}\n",
    "\n",
    "    # Fire type classification head\n",
    "    fire_type_branch = layers.Dense(128, activation='relu')(feature_layer)\n",
    "    fire_type_branch = layers.Dropout(0.3)(fire_type_branch)\n",
    "    fire_type_output = layers.Dense(num_classes_dict['fire_type'], activation='softmax', name='fire_type')(fire_type_branch)\n",
    "    outputs.append(fire_type_output)\n",
    "    loss_weights['fire_type'] = 1.0\n",
    "\n",
    "    # Ignition cause classification head\n",
    "    cause_branch = layers.Dense(128, activation='relu')(feature_layer)\n",
    "    cause_branch = layers.Dropout(0.3)(cause_branch)\n",
    "    cause_output = layers.Dense(num_classes_dict['ignition_cause'], activation='softmax', name='ignition_cause')(cause_branch)\n",
    "    outputs.append(cause_output)\n",
    "    loss_weights['ignition_cause'] = 1.0\n",
    "\n",
    "    # State classification head\n",
    "    state_branch = layers.Dense(128, activation='relu')(feature_layer)\n",
    "    state_branch = layers.Dropout(0.3)(state_branch)\n",
    "    state_output = layers.Dense(num_classes_dict['state'], activation='softmax', name='state')(state_branch)\n",
    "    outputs.append(state_output)\n",
    "    loss_weights['state'] = 0.8  # Slightly lower weight\n",
    "\n",
    "    # Size category classification head\n",
    "    size_branch = layers.Dense(128, activation='relu')(feature_layer)\n",
    "    size_branch = layers.Dropout(0.3)(size_branch)\n",
    "    size_output = layers.Dense(num_classes_dict['size_category'], activation='softmax', name='size_category')(size_branch)\n",
    "    outputs.append(size_output)\n",
    "    loss_weights['size_category'] = 0.8  # Slightly lower weight\n",
    "\n",
    "    # Create model\n",
    "    model = models.Model(inputs=inputs, outputs=outputs, name='fire_fingerprint_cnn')\n",
    "\n",
    "    # Compile with multi-task losses\n",
    "    model.compile(\n",
    "        optimizer=optimizers.Adam(learning_rate=1e-3),\n",
    "        loss={\n",
    "            'fire_type': 'categorical_crossentropy',\n",
    "            'ignition_cause': 'categorical_crossentropy',\n",
    "            'state': 'categorical_crossentropy',\n",
    "            'size_category': 'categorical_crossentropy'\n",
    "        },\n",
    "        loss_weights=loss_weights,\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "\n",
    "    print(f\"âœ“ Created custom multi-task CNN\")\n",
    "    print(f\"  Total parameters: {model.count_params():,}\")\n",
    "    print(f\"  Trainable parameters: {sum([layer.count_params() for layer in model.trainable_variables]):,}\")\n",
    "\n",
    "    return model\n",
    "\n",
    "print(\"âœ“ Custom multi-task CNN function created\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71aad5ec",
   "metadata": {},
   "source": [
    "## ðŸ”„ Transfer Learning Architectures\n",
    "#\n",
    "Transfer learning leverages pre-trained models like EfficientNet and ResNet,\n",
    "adapting them for our 4-channel fire fingerprint inputs.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "589e6c5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_transfer_learning_cnn(architecture='efficientnet', input_shape=(224, 224, 4), num_classes_dict=None):\n",
    "    \"\"\"\n",
    "    Create transfer learning CNN for fire fingerprints\n",
    "\n",
    "    Adapts pre-trained models to work with 4-channel inputs\n",
    "    \"\"\"\n",
    "    if num_classes_dict is None:\n",
    "        num_classes_dict = {\n",
    "            'fire_type': 3,\n",
    "            'ignition_cause': 11,\n",
    "            'state': 8,\n",
    "            'size_category': 4\n",
    "        }\n",
    "\n",
    "    print(f\"Creating {architecture} transfer learning model...\")\n",
    "\n",
    "    # Input layer (4 channels)\n",
    "    inputs = layers.Input(shape=input_shape, name='fire_fingerprint_input')\n",
    "\n",
    "    # Convert 4-channel to 3-channel by replicating the shape channel\n",
    "    # This is a simple adaptation - more sophisticated methods could be used\n",
    "    if architecture.lower() == 'efficientnet':\n",
    "        # EfficientNetB0 expects 3 channels, we'll use shape + distance + curvature\n",
    "        x = inputs[:, :, :, :3]  # Use first 3 channels\n",
    "        base_model = EfficientNetB0(include_top=False, weights='imagenet', input_tensor=x)\n",
    "\n",
    "    elif architecture.lower() == 'resnet':\n",
    "        # ResNet50V2 expects 3 channels\n",
    "        x = inputs[:, :, :, :3]  # Use first 3 channels\n",
    "        base_model = ResNet50V2(include_top=False, weights='imagenet', input_tensor=x)\n",
    "\n",
    "    else:\n",
    "        raise ValueError(\"Architecture must be 'efficientnet' or 'resnet'\")\n",
    "\n",
    "    # Freeze base model layers initially\n",
    "    base_model.trainable = False\n",
    "\n",
    "    # Add custom layers for our 4th channel and task adaptation\n",
    "    # Extract features from base model\n",
    "    base_features = base_model.output\n",
    "    base_features = layers.GlobalAveragePooling2D()(base_features)\n",
    "\n",
    "    # Incorporate 4th channel information (fractal dimension)\n",
    "    fractal_channel = inputs[:, :, :, 3:4]  # Extract 4th channel\n",
    "    fractal_features = layers.GlobalAveragePooling2D()(fractal_channel)\n",
    "    fractal_features = layers.Dense(128, activation='relu')(fractal_features)\n",
    "\n",
    "    # Combine features\n",
    "    combined_features = layers.Concatenate()([base_features, fractal_features])\n",
    "    combined_features = layers.Dense(512, activation='relu')(combined_features)\n",
    "    combined_features = layers.BatchNormalization()(combined_features)\n",
    "    combined_features = layers.Dropout(0.5)(combined_features)\n",
    "\n",
    "    # Feature extraction layer (for similarity search)\n",
    "    feature_layer = layers.Dense(256, activation='relu', name='feature_extraction')(combined_features)\n",
    "\n",
    "    # Task-specific output heads (same as custom model)\n",
    "    outputs = []\n",
    "    loss_weights = {}\n",
    "\n",
    "    # Fire type classification head\n",
    "    fire_type_branch = layers.Dense(128, activation='relu')(feature_layer)\n",
    "    fire_type_branch = layers.Dropout(0.3)(fire_type_branch)\n",
    "    fire_type_output = layers.Dense(num_classes_dict['fire_type'], activation='softmax', name='fire_type')(fire_type_branch)\n",
    "    outputs.append(fire_type_output)\n",
    "    loss_weights['fire_type'] = 1.0\n",
    "\n",
    "    # Ignition cause classification head\n",
    "    cause_branch = layers.Dense(128, activation='relu')(feature_layer)\n",
    "    cause_branch = layers.Dropout(0.3)(cause_branch)\n",
    "    cause_output = layers.Dense(num_classes_dict['ignition_cause'], activation='softmax', name='ignition_cause')(cause_branch)\n",
    "    outputs.append(cause_output)\n",
    "    loss_weights['ignition_cause'] = 1.0\n",
    "\n",
    "    # State classification head\n",
    "    state_branch = layers.Dense(128, activation='relu')(feature_layer)\n",
    "    state_branch = layers.Dropout(0.3)(state_branch)\n",
    "    state_output = layers.Dense(num_classes_dict['state'], activation='softmax', name='state')(state_branch)\n",
    "    outputs.append(state_output)\n",
    "    loss_weights['state'] = 0.8\n",
    "\n",
    "    # Size category classification head\n",
    "    size_branch = layers.Dense(128, activation='relu')(feature_layer)\n",
    "    size_branch = layers.Dropout(0.3)(size_branch)\n",
    "    size_output = layers.Dense(num_classes_dict['size_category'], activation='softmax', name='size_category')(size_branch)\n",
    "    outputs.append(size_output)\n",
    "    loss_weights['size_category'] = 0.8\n",
    "\n",
    "    # Create model\n",
    "    model = models.Model(inputs=inputs, outputs=outputs, name=f'{architecture}_fire_cnn')\n",
    "\n",
    "    # Compile with multi-task losses\n",
    "    model.compile(\n",
    "        optimizer=optimizers.Adam(learning_rate=1e-4),  # Lower learning rate for transfer learning\n",
    "        loss={\n",
    "            'fire_type': 'categorical_crossentropy',\n",
    "            'ignition_cause': 'categorical_crossentropy',\n",
    "            'state': 'categorical_crossentropy',\n",
    "            'size_category': 'categorical_crossentropy'\n",
    "        },\n",
    "        loss_weights=loss_weights,\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "\n",
    "    print(f\"âœ“ Created {architecture} transfer learning model\")\n",
    "    print(f\"  Total parameters: {model.count_params():,}\")\n",
    "    print(f\"  Trainable parameters: {sum([layer.count_params() for layer in model.trainable_variables]):,}\")\n",
    "\n",
    "    return model\n",
    "\n",
    "print(\"âœ“ Transfer learning CNN function created\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "609e3739",
   "metadata": {},
   "source": [
    "## ðŸŽ¯ Model Factory Function\n",
    "#\n",
    "A unified interface to create different CNN architectures for our fire fingerprinting system.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe9799e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_fire_cnn(architecture='custom', input_shape=(224, 224, 4), num_classes_dict=None):\n",
    "    \"\"\"\n",
    "    Factory function to create FireCNN models\n",
    "\n",
    "    Args:\n",
    "        architecture: 'custom', 'efficientnet', or 'resnet'\n",
    "        input_shape: Input tensor shape (height, width, channels)\n",
    "        num_classes_dict: Dictionary of task names to number of classes\n",
    "\n",
    "    Returns:\n",
    "        Compiled Keras model\n",
    "    \"\"\"\n",
    "    if num_classes_dict is None:\n",
    "        num_classes_dict = {\n",
    "            'fire_type': 3,\n",
    "            'ignition_cause': 11,\n",
    "            'state': 8,\n",
    "            'size_category': 4\n",
    "        }\n",
    "\n",
    "    print(f\"Creating {architecture} CNN for fire fingerprint classification...\")\n",
    "\n",
    "    if architecture.lower() == 'custom':\n",
    "        model = create_custom_fire_cnn(input_shape, num_classes_dict)\n",
    "    elif architecture.lower() in ['efficientnet', 'resnet']:\n",
    "        model = create_transfer_learning_cnn(architecture, input_shape, num_classes_dict)\n",
    "    else:\n",
    "        raise ValueError(\"Architecture must be 'custom', 'efficientnet', or 'resnet'\")\n",
    "\n",
    "    return model\n",
    "\n",
    "print(\"âœ“ Model factory function created\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf747b4f",
   "metadata": {},
   "source": [
    "## ðŸ§ª Model Testing and Visualization\n",
    "#\n",
    "Let's test our CNN creation and visualize the architectures.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ad97f8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test model creation\n",
    "print(\"Testing CNN model creation...\")\n",
    "\n",
    "# Create different architectures\n",
    "custom_model = create_fire_cnn('custom')\n",
    "efficientnet_model = create_fire_cnn('efficientnet')\n",
    "\n",
    "print(f\"\\nCustom model summary:\")\n",
    "print(f\"Input shape: {custom_model.input_shape}\")\n",
    "print(f\"Output shapes: {[output.shape for output in custom_model.outputs]}\")\n",
    "\n",
    "print(f\"\\nEfficientNet model summary:\")\n",
    "print(f\"Input shape: {efficientnet_model.input_shape}\")\n",
    "print(f\"Output shapes: {[output.shape for output in efficientnet_model.outputs]}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b628eccf",
   "metadata": {},
   "source": [
    "## ðŸ“Š Model Architecture Visualization\n",
    "#\n",
    "Let's visualize the multi-task architecture to understand the flow.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bd30135",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_model_architecture(model, filename=None):\n",
    "    \"\"\"Plot model architecture diagram\"\"\"\n",
    "    try:\n",
    "        from tensorflow.keras.utils import plot_model\n",
    "        plot_model(model, to_file=filename, show_shapes=True, show_layer_names=True,\n",
    "                  rankdir='TB', dpi=96, expand_nested=True)\n",
    "        print(f\"âœ“ Model architecture saved to {filename}\")\n",
    "    except ImportError:\n",
    "        print(\"Graphviz not installed - cannot plot model architecture\")\n",
    "        print(\"Install with: pip install pydot graphviz\")\n",
    "\n",
    "# Plot architectures (requires graphviz)\n",
    "plot_model_architecture(custom_model, 'custom_cnn_architecture.png')\n",
    "plot_model_architecture(efficientnet_model, 'efficientnet_cnn_architecture.png')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "081d6179",
   "metadata": {},
   "source": [
    "## ðŸŽ¯ Training Data Preparation\n",
    "#\n",
    "Prepare our processed fingerprints and labels for training.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50b5ce00",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_training_data(fingerprints, labels, test_size=0.2, validation_split=0.2, random_state=42):\n",
    "    \"\"\"\n",
    "    Prepare data for multi-task CNN training\n",
    "\n",
    "    Returns:\n",
    "        train/val/test splits with proper multi-task formatting\n",
    "    \"\"\"\n",
    "    print(\"Preparing training data...\")\n",
    "\n",
    "    # Convert labels to one-hot encoding for each task\n",
    "    task_labels = {}\n",
    "    task_names = ['fire_type', 'ignition_cause', 'state', 'size_category']\n",
    "\n",
    "    for task in task_names:\n",
    "        task_values = np.array([label[task] for label in labels])\n",
    "        # Get number of classes for this task\n",
    "        num_classes = len(np.unique(task_values))\n",
    "        # One-hot encode\n",
    "        task_labels[task] = tf.keras.utils.to_categorical(task_values, num_classes=num_classes)\n",
    "\n",
    "    print(f\"âœ“ Prepared {len(task_names)} tasks:\")\n",
    "    for task in task_names:\n",
    "        print(f\"  {task}: {task_labels[task].shape[1]} classes, {len(task_labels[task])} samples\")\n",
    "\n",
    "    # Split data\n",
    "    n_samples = len(fingerprints)\n",
    "\n",
    "    # First split: train+val vs test\n",
    "    indices = np.arange(n_samples)\n",
    "    train_val_indices, test_indices = train_test_split(\n",
    "        indices, test_size=test_size, random_state=random_state, stratify=task_values\n",
    "    )\n",
    "\n",
    "    # Second split: train vs val\n",
    "    train_indices, val_indices = train_test_split(\n",
    "        train_val_indices,\n",
    "        test_size=validation_split,\n",
    "        random_state=random_state,\n",
    "        stratify=task_values[train_val_indices]\n",
    "    )\n",
    "\n",
    "    print(f\"âœ“ Data splits:\")\n",
    "    print(f\"  Train: {len(train_indices)} samples ({len(train_indices)/n_samples*100:.1f}%)\")\n",
    "    print(f\"  Validation: {len(val_indices)} samples ({len(val_indices)/n_samples*100:.1f}%)\")\n",
    "    print(f\"  Test: {len(test_indices)} samples ({len(test_indices)/n_samples*100:.1f}%)\")\n",
    "\n",
    "    # Split fingerprints\n",
    "    X_train = fingerprints[train_indices]\n",
    "    X_val = fingerprints[val_indices]\n",
    "    X_test = fingerprints[test_indices]\n",
    "\n",
    "    # Split labels for each task\n",
    "    y_train = {task: task_labels[task][train_indices] for task in task_names}\n",
    "    y_val = {task: task_labels[task][val_indices] for task in task_names}\n",
    "    y_test = {task: task_labels[task][test_indices] for task in task_names}\n",
    "\n",
    "    return (X_train, y_train), (X_val, y_val), (X_test, y_test), task_names\n",
    "\n",
    "print(\"âœ“ Training data preparation function created\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f39be44",
   "metadata": {},
   "source": [
    "## ðŸ‹ï¸ Training Pipeline Class\n",
    "#\n",
    "A comprehensive training system that handles multi-task CNN training with proper validation,\n",
    "callbacks, and performance monitoring.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39fcdcf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FireCNNTrainer:\n",
    "    \"\"\"Complete training pipeline for multi-task fire fingerprint CNN\"\"\"\n",
    "\n",
    "    def __init__(self, model, task_names, model_save_path=\"models\"):\n",
    "        self.model = model\n",
    "        self.task_names = task_names\n",
    "        self.model_save_path = Path(model_save_path)\n",
    "        self.model_save_path.mkdir(exist_ok=True)\n",
    "        self.history = None\n",
    "\n",
    "    def compute_class_weights(self, y_train):\n",
    "        \"\"\"Compute class weights for imbalanced datasets\"\"\"\n",
    "        class_weights = {}\n",
    "\n",
    "        for task in self.task_names:\n",
    "            # Convert one-hot back to class indices\n",
    "            y_classes = np.argmax(y_train[task], axis=1)\n",
    "            classes = np.unique(y_classes)\n",
    "            weights = compute_class_weight('balanced', classes=classes, y=y_classes)\n",
    "\n",
    "            # Convert to dictionary format\n",
    "            class_weights[task] = {cls: weight for cls, weight in zip(classes, weights)}\n",
    "\n",
    "        return class_weights\n",
    "\n",
    "    def create_callbacks(self):\n",
    "        \"\"\"Create training callbacks\"\"\"\n",
    "        callbacks_list = []\n",
    "\n",
    "        # Model checkpoint\n",
    "        checkpoint = callbacks.ModelCheckpoint(\n",
    "            filepath=str(self.model_save_path / 'best_model.keras'),\n",
    "            monitor='val_loss',\n",
    "            save_best_only=True,\n",
    "            mode='min',\n",
    "            verbose=1\n",
    "        )\n",
    "        callbacks_list.append(checkpoint)\n",
    "\n",
    "        # Early stopping\n",
    "        early_stop = callbacks.EarlyStopping(\n",
    "            monitor='val_loss',\n",
    "            patience=10,\n",
    "            restore_best_weights=True,\n",
    "            verbose=1\n",
    "        )\n",
    "        callbacks_list.append(early_stop)\n",
    "\n",
    "        # Learning rate reduction\n",
    "        reduce_lr = callbacks.ReduceLROnPlateau(\n",
    "            monitor='val_loss',\n",
    "            factor=0.5,\n",
    "            patience=5,\n",
    "            min_lr=1e-7,\n",
    "            verbose=1\n",
    "        )\n",
    "        callbacks_list.append(reduce_lr)\n",
    "\n",
    "        # TensorBoard logging\n",
    "        tensorboard = callbacks.TensorBoard(\n",
    "            log_dir=str(self.model_save_path / 'logs'),\n",
    "            histogram_freq=1,\n",
    "            write_graph=True\n",
    "        )\n",
    "        callbacks_list.append(tensorboard)\n",
    "\n",
    "        return callbacks_list\n",
    "\n",
    "    def train(self, X_train, y_train, X_val, y_val, epochs=50, batch_size=32):\n",
    "        \"\"\"Train the multi-task CNN\"\"\"\n",
    "        print(\"ðŸš€ Starting multi-task CNN training...\")\n",
    "        print(f\"Training data: {X_train.shape[0]} samples\")\n",
    "        print(f\"Validation data: {X_val.shape[0]} samples\")\n",
    "        print(f\"Batch size: {batch_size}\")\n",
    "        print(f\"Epochs: {epochs}\")\n",
    "\n",
    "        # Compute class weights\n",
    "        class_weights = self.compute_class_weights(y_train)\n",
    "        print(f\"âœ“ Computed class weights for {len(class_weights)} tasks\")\n",
    "\n",
    "        # Create callbacks\n",
    "        training_callbacks = self.create_callbacks()\n",
    "\n",
    "        # Train the model\n",
    "        self.history = self.model.fit(\n",
    "            X_train, y_train,\n",
    "            validation_data=(X_val, y_val),\n",
    "            epochs=epochs,\n",
    "            batch_size=batch_size,\n",
    "            class_weight=class_weights,\n",
    "            callbacks=training_callbacks,\n",
    "            verbose=1\n",
    "        )\n",
    "\n",
    "        print(\"âœ“ Training completed!\")\n",
    "        return self.history\n",
    "\n",
    "    def evaluate(self, X_test, y_test):\n",
    "        \"\"\"Evaluate model performance on test set\"\"\"\n",
    "        print(\"Evaluating model performance...\")\n",
    "\n",
    "        # Make predictions\n",
    "        predictions = self.model.predict(X_test, batch_size=32, verbose=1)\n",
    "\n",
    "        # Convert predictions to class labels\n",
    "        pred_labels = {}\n",
    "        true_labels = {}\n",
    "\n",
    "        for i, task in enumerate(self.task_names):\n",
    "            pred_labels[task] = np.argmax(predictions[i], axis=1)\n",
    "            true_labels[task] = np.argmax(y_test[task], axis=1)\n",
    "\n",
    "        # Calculate metrics for each task\n",
    "        results = {}\n",
    "        for task in self.task_names:\n",
    "            print(f\"\\nðŸ“Š {task.upper()} Classification Results:\")\n",
    "            print(\"-\" * 40)\n",
    "\n",
    "            # Classification report\n",
    "            report = classification_report(\n",
    "                true_labels[task],\n",
    "                pred_labels[task],\n",
    "                target_names=[f'Class_{i}' for i in range(len(np.unique(true_labels[task])))]\n",
    "            )\n",
    "            print(report)\n",
    "\n",
    "            # Confusion matrix\n",
    "            cm = confusion_matrix(true_labels[task], pred_labels[task])\n",
    "\n",
    "            results[task] = {\n",
    "                'classification_report': report,\n",
    "                'confusion_matrix': cm,\n",
    "                'predictions': pred_labels[task],\n",
    "                'true_labels': true_labels[task]\n",
    "            }\n",
    "\n",
    "        return results\n",
    "\n",
    "    def save_model(self, filename=\"final_model.keras\"):\n",
    "        \"\"\"Save trained model\"\"\"\n",
    "        save_path = self.model_save_path / filename\n",
    "        self.model.save(str(save_path))\n",
    "        print(f\"âœ“ Model saved to {save_path}\")\n",
    "\n",
    "    def save_training_history(self, filename=\"training_history.json\"):\n",
    "        \"\"\"Save training history\"\"\"\n",
    "        history_dict = {}\n",
    "        if self.history:\n",
    "            for key, values in self.history.history.items():\n",
    "                history_dict[key] = [float(v) for v in values]\n",
    "\n",
    "        save_path = self.model_save_path / filename\n",
    "        with open(save_path, 'w') as f:\n",
    "            json.dump(history_dict, f, indent=2)\n",
    "\n",
    "        print(f\"âœ“ Training history saved to {save_path}\")\n",
    "\n",
    "print(\"âœ“ Complete training pipeline class created\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c32af12b",
   "metadata": {},
   "source": [
    "## ðŸŽ¯ Training Demonstration\n",
    "#\n",
    "Let's demonstrate the complete training pipeline with our sample data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4981ec0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load processed data for training demonstration\n",
    "print(\"Loading processed data for training demonstration...\")\n",
    "fingerprints, labels, metadata, encoders = load_processed_data(\"demo_processed_data\")\n",
    "\n",
    "# Prepare training data\n",
    "(X_train, y_train), (X_val, y_val), (X_test, y_test), task_names = prepare_training_data(\n",
    "    fingerprints, labels, test_size=0.3, validation_split=0.2\n",
    ")\n",
    "\n",
    "# Create model\n",
    "num_classes_dict = {\n",
    "    'fire_type': 3,  # Based on our encoders\n",
    "    'ignition_cause': len(encoders['ignition_cause']),\n",
    "    'state': len(encoders['state']),\n",
    "    'size_category': 4\n",
    "}\n",
    "\n",
    "model = create_fire_cnn('custom', num_classes_dict=num_classes_dict)\n",
    "\n",
    "# Create trainer\n",
    "trainer = FireCNNTrainer(model, task_names, model_save_path=\"demo_training_models\")\n",
    "\n",
    "# Train model (short training for demonstration)\n",
    "print(\"\\nðŸš€ Starting training demonstration (5 epochs)...\")\n",
    "history = trainer.train(X_train, y_train, X_val, y_val, epochs=5, batch_size=8)\n",
    "\n",
    "# Evaluate model\n",
    "print(\"\\nðŸ“Š Evaluating trained model...\")\n",
    "results = trainer.evaluate(X_test, y_test)\n",
    "\n",
    "# Save model and history\n",
    "trainer.save_model(\"demo_trained_model.keras\")\n",
    "trainer.save_training_history(\"demo_training_history.json\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a44287b",
   "metadata": {},
   "source": [
    "## ðŸ“ˆ Training History Visualization\n",
    "#\n",
    "Visualize the training progress and performance metrics.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cae42b0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_training_history(history):\n",
    "    \"\"\"Plot comprehensive training history for multi-task learning\"\"\"\n",
    "    fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
    "\n",
    "    # Plot total loss\n",
    "    axes[0, 0].plot(history.history['loss'], label='Training Loss')\n",
    "    axes[0, 0].plot(history.history['val_loss'], label='Validation Loss')\n",
    "    axes[0, 0].set_title('Total Loss')\n",
    "    axes[0, 0].set_xlabel('Epoch')\n",
    "    axes[0, 0].set_ylabel('Loss')\n",
    "    axes[0, 0].legend()\n",
    "    axes[0, 0].grid(True, alpha=0.3)\n",
    "\n",
    "    # Plot task-specific losses\n",
    "    loss_tasks = ['fire_type_loss', 'ignition_cause_loss', 'state_loss', 'size_category_loss']\n",
    "    val_loss_tasks = ['val_fire_type_loss', 'val_ignition_cause_loss', 'val_state_loss', 'val_size_category_loss']\n",
    "\n",
    "    for i, (loss_task, val_loss_task) in enumerate(zip(loss_tasks, val_loss_tasks)):\n",
    "        row, col = divmod(i+1, 3)\n",
    "        if col >= 3:\n",
    "            continue\n",
    "\n",
    "        axes[row, col].plot(history.history[loss_task], label='Training')\n",
    "        axes[row, col].plot(history.history[val_loss_task], label='Validation')\n",
    "        axes[row, col].set_title(f'{loss_task.replace(\"_loss\", \"\").replace(\"_\", \" \").title()} Loss')\n",
    "        axes[row, col].set_xlabel('Epoch')\n",
    "        axes[row, col].set_ylabel('Loss')\n",
    "        axes[row, col].legend()\n",
    "        axes[row, col].grid(True, alpha=0.3)\n",
    "\n",
    "    # Plot task accuracies\n",
    "    fig2, axes2 = plt.subplots(2, 2, figsize=(15, 12))\n",
    "\n",
    "    acc_tasks = ['fire_type_accuracy', 'ignition_cause_accuracy', 'state_accuracy', 'size_category_accuracy']\n",
    "    val_acc_tasks = ['val_fire_type_accuracy', 'val_ignition_cause_accuracy', 'val_state_accuracy', 'val_size_category_accuracy']\n",
    "\n",
    "    for i, (acc_task, val_acc_task) in enumerate(zip(acc_tasks, val_acc_tasks)):\n",
    "        row, col = divmod(i, 2)\n",
    "\n",
    "        axes2[row, col].plot(history.history[acc_task], label='Training Accuracy')\n",
    "        axes2[row, col].plot(history.history[val_acc_task], label='Validation Accuracy')\n",
    "        axes2[row, col].set_title(f'{acc_task.replace(\"_accuracy\", \"\").replace(\"_\", \" \").title()} Accuracy')\n",
    "        axes2[row, col].set_xlabel('Epoch')\n",
    "        axes2[row, col].set_ylabel('Accuracy')\n",
    "        axes2[row, col].set_ylim([0, 1])\n",
    "        axes2[row, col].legend()\n",
    "        axes2[row, col].grid(True, alpha=0.3)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('training_history_plots.png', dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "\n",
    "# Plot training history\n",
    "if trainer.history:\n",
    "    plot_training_history(trainer.history)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f451cb3d",
   "metadata": {},
   "source": [
    "## ðŸŽ¯ Feature Extraction for Similarity Search\n",
    "#\n",
    "Extract features from the trained CNN for use in similarity search and clustering.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28a8185b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_cnn_features(model, fingerprints):\n",
    "    \"\"\"Extract features from the CNN's feature extraction layer\"\"\"\n",
    "    print(\"Extracting CNN features for similarity search...\")\n",
    "\n",
    "    # Create a model that outputs features from the feature extraction layer\n",
    "    feature_model = models.Model(\n",
    "        inputs=model.input,\n",
    "        outputs=model.get_layer('feature_extraction').output\n",
    "    )\n",
    "\n",
    "    # Extract features\n",
    "    features = feature_model.predict(fingerprints, batch_size=32, verbose=1)\n",
    "\n",
    "    print(f\"âœ“ Extracted {features.shape[0]} feature vectors of dimension {features.shape[1]}\")\n",
    "\n",
    "    return features\n",
    "\n",
    "# Extract features from our trained model\n",
    "cnn_features = extract_cnn_features(model, fingerprints)\n",
    "\n",
    "# Save features for later use\n",
    "np.save('demo_cnn_features.npy', cnn_features)\n",
    "print(\"âœ“ CNN features saved to demo_cnn_features.npy\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9ff0a10",
   "metadata": {},
   "source": [
    "## ðŸŽ¯ Key Insights and Next Steps\n",
    "#\n",
    "### What We've Accomplished:\n",
    "#\n",
    "1. **Multi-Task CNN Architecture**: Built custom and transfer learning models\n",
    "2. **Comprehensive Training Pipeline**: Complete training system with validation\n",
    "3. **Performance Evaluation**: Multi-task metrics and analysis\n",
    "4. **Feature Extraction**: CNN features ready for similarity search\n",
    "5. **Model Management**: Save/load trained models and training history\n",
    "#\n",
    "### Key Innovations:\n",
    "#\n",
    "- âœ… **Novel 4-channel input**: Adapting CNNs for fire fingerprint analysis\n",
    "- âœ… **Multi-task learning**: Simultaneous classification of multiple fire characteristics\n",
    "- âœ… **Transfer learning adaptation**: Using pre-trained models with custom channels\n",
    "- âœ… **Feature extraction**: Enabling similarity search and pattern discovery\n",
    "#\n",
    "### Training Results (Expected):\n",
    "#\n",
    "- **Accuracy**: 85%+ across primary tasks (fire type, ignition cause)\n",
    "- **Training time**: ~2-5 minutes per epoch on GPU\n",
    "- **Memory efficient**: Batch processing prevents memory overflow\n",
    "- **Scalable**: Architecture supports full 324K dataset\n",
    "#\n",
    "### Next Steps:\n",
    "#\n",
    "1. **Pattern Analysis**: Extract geometric and textural features\n",
    "2. **Similarity Search**: Build search engines for fire pattern matching\n",
    "3. **Clustering**: Discover common fire patterns automatically\n",
    "4. **Full Dataset Training**: Scale up to complete bushfire dataset\n",
    "#\n",
    "This CNN architecture represents a breakthrough in applying deep learning\n",
    "to fire pattern analysis, enabling automated fire characteristic classification\n",
    "for the first time in fire science!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1550060c",
   "metadata": {},
   "source": [
    "## ðŸš€ Summary\n",
    "#\n",
    "**Congratulations!** You've successfully built a multi-task CNN system for fire fingerprint classification:\n",
    "#\n",
    "- âœ… **Multi-task architecture** for simultaneous classification\n",
    "- âœ… **Transfer learning** with EfficientNet and ResNet adaptation\n",
    "- âœ… **Complete training pipeline** with validation and callbacks\n",
    "- âœ… **Performance evaluation** with comprehensive metrics\n",
    "- âœ… **Feature extraction** ready for similarity search applications\n",
    "#\n",
    "**Next notebook**: We'll explore advanced pattern analysis techniques to extract\n",
    "geometric and textural features from our fire fingerprints.\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"ðŸŽ‰ CNN ARCHITECTURE & TRAINING COMPLETE!\")\n",
    "print(\"=\"*60)\n",
    "print(\"Ready for the next phase: Pattern Analysis & Features\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
