{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "32aee08b",
   "metadata": {},
   "source": [
    "# ðŸ” Pattern Analysis & Feature Extraction\n",
    "#\n",
    "## Uncovering Geometric and Textural Fire Patterns\n",
    "#\n",
    "This notebook explores advanced pattern analysis techniques to extract rich geometric\n",
    "and textural features from our 4-channel fire fingerprints. These features enable\n",
    "deeper understanding of fire behavior and support similarity search applications.\n",
    "#\n",
    "**Features**: 20+ geometric, complexity, texture, and curvature-based descriptors\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f8ee057",
   "metadata": {},
   "source": [
    "## ðŸ“‹ What You'll Learn\n",
    "#\n",
    "1. **Geometric Features**: Shape-based fire characteristics\n",
    "2. **Complexity Analysis**: Fractal dimensions and boundary roughness\n",
    "3. **Texture Features**: GLCM analysis of distance transforms\n",
    "4. **Curvature Analysis**: Boundary complexity and burning patterns\n",
    "5. **Feature Visualization**: Understanding feature distributions and correlations\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f331911c",
   "metadata": {},
   "source": [
    "## ðŸ› ï¸ Setup and Imports\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c49bd1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "from scipy import ndimage\n",
    "from scipy.spatial.distance import pdist, squareform\n",
    "from scipy.stats import entropy\n",
    "from skimage import measure, morphology\n",
    "from skimage.feature import graycomatrix, graycoprops, local_binary_pattern\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import json\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Import our previous components\n",
    "exec(open('02_Data_Processing_Pipeline.py').read())\n",
    "\n",
    "print(\"ðŸ”¥ Fire Fingerprinting System - Pattern Analysis & Features\")\n",
    "print(\"=\" * 60)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef5e9636",
   "metadata": {},
   "source": [
    "## ðŸ—ï¸ Fire Pattern Analyzer Class\n",
    "#\n",
    "A comprehensive feature extraction system for fire fingerprints that computes\n",
    "20+ different geometric and textural features.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "549a1732",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FirePatternAnalyzer:\n",
    "    \"\"\"Analyze fire patterns and extract features from fingerprints\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self.feature_names = []\n",
    "        self.feature_descriptions = {}\n",
    "        self._initialize_feature_definitions()\n",
    "\n",
    "    def _initialize_feature_definitions(self):\n",
    "        \"\"\"Initialize feature names and descriptions\"\"\"\n",
    "        self.feature_descriptions = {\n",
    "            # Shape-based features\n",
    "            'area': 'Total area of fire boundary',\n",
    "            'perimeter': 'Total perimeter length',\n",
    "            'compactness': 'Compactness ratio (4Ï€*area/perimeterÂ²)',\n",
    "            'elongation': 'Elongation ratio (major/minor axis)',\n",
    "            'solidity': 'Ratio of area to convex hull area',\n",
    "            'extent': 'Ratio of area to bounding box area',\n",
    "            'eccentricity': 'Eccentricity of fitted ellipse',\n",
    "            'orientation': 'Orientation of major axis',\n",
    "\n",
    "            # Complexity features\n",
    "            'fractal_dimension': 'Fractal dimension of boundary',\n",
    "            'boundary_roughness': 'Roughness of fire boundary',\n",
    "            'convexity_defects': 'Number of convexity defects',\n",
    "            'shape_complexity': 'Overall shape complexity measure',\n",
    "\n",
    "            # Texture features (from distance transform)\n",
    "            'texture_contrast': 'Contrast in distance transform',\n",
    "            'texture_homogeneity': 'Homogeneity in distance transform',\n",
    "            'texture_energy': 'Energy in distance transform',\n",
    "            'texture_correlation': 'Correlation in distance transform',\n",
    "\n",
    "            # Curvature features\n",
    "            'mean_curvature': 'Mean boundary curvature',\n",
    "            'curvature_variance': 'Variance in boundary curvature',\n",
    "            'max_curvature': 'Maximum boundary curvature',\n",
    "            'curvature_peaks': 'Number of curvature peaks',\n",
    "\n",
    "            # Multi-scale features\n",
    "            'multi_scale_area': 'Area at different scales',\n",
    "            'multi_scale_perimeter': 'Perimeter at different scales',\n",
    "            'multi_scale_complexity': 'Complexity at different scales'\n",
    "        }\n",
    "\n",
    "        self.feature_names = list(self.feature_descriptions.keys())\n",
    "\n",
    "    # Shape-based features\n",
    "    def extract_shape_features(self, fingerprint):\n",
    "        \"\"\"Extract basic shape features from fire fingerprint\"\"\"\n",
    "        shape_mask = fingerprint[:, :, 0]  # Binary shape channel\n",
    "\n",
    "        features = {}\n",
    "\n",
    "        # Basic shape properties\n",
    "        contours, _ = cv2.findContours(\n",
    "            (shape_mask * 255).astype(np.uint8),\n",
    "            cv2.RETR_EXTERNAL,\n",
    "            cv2.CHAIN_APPROX_SIMPLE\n",
    "        )\n",
    "\n",
    "        if len(contours) > 0:\n",
    "            # Get largest contour\n",
    "            main_contour = max(contours, key=cv2.contourArea)\n",
    "\n",
    "            # Area and perimeter\n",
    "            features['area'] = cv2.contourArea(main_contour)\n",
    "            features['perimeter'] = cv2.arcLength(main_contour, True)\n",
    "\n",
    "            # Compactness (circularity)\n",
    "            if features['perimeter'] > 0:\n",
    "                features['compactness'] = 4 * np.pi * features['area'] / (features['perimeter'] ** 2)\n",
    "            else:\n",
    "                features['compactness'] = 0\n",
    "\n",
    "            # Fit ellipse for elongation and orientation\n",
    "            if len(main_contour) >= 5:  # Need at least 5 points for ellipse fitting\n",
    "                ellipse = cv2.fitEllipse(main_contour)\n",
    "                (center, axes, angle) = ellipse\n",
    "                major_axis, minor_axis = axes\n",
    "\n",
    "                features['elongation'] = major_axis / minor_axis if minor_axis > 0 else 0\n",
    "                features['orientation'] = angle\n",
    "\n",
    "                # Eccentricity\n",
    "                features['eccentricity'] = np.sqrt(1 - (minor_axis/major_axis)**2) if major_axis > 0 else 0\n",
    "            else:\n",
    "                features['elongation'] = 0\n",
    "                features['orientation'] = 0\n",
    "                features['eccentricity'] = 0\n",
    "\n",
    "            # Convex hull properties\n",
    "            hull = cv2.convexHull(main_contour)\n",
    "            hull_area = cv2.contourArea(hull)\n",
    "            features['solidity'] = features['area'] / hull_area if hull_area > 0 else 0\n",
    "\n",
    "            # Bounding box\n",
    "            x, y, w, h = cv2.boundingRect(main_contour)\n",
    "            bounding_area = w * h\n",
    "            features['extent'] = features['area'] / bounding_area if bounding_area > 0 else 0\n",
    "\n",
    "        return features\n",
    "\n",
    "    # Complexity features\n",
    "    def extract_complexity_features(self, fingerprint):\n",
    "        \"\"\"Extract complexity features from fire fingerprint\"\"\"\n",
    "        shape_mask = fingerprint[:, :, 0]\n",
    "        distance_transform = fingerprint[:, :, 1]\n",
    "\n",
    "        features = {}\n",
    "\n",
    "        # Fractal dimension using box counting\n",
    "        features['fractal_dimension'] = self._calculate_fractal_dimension(shape_mask)\n",
    "\n",
    "        # Boundary roughness (standard deviation of boundary distances)\n",
    "        contours, _ = cv2.findContours(\n",
    "            (shape_mask * 255).astype(np.uint8),\n",
    "            cv2.RETR_EXTERNAL,\n",
    "            cv2.CHAIN_APPROX_NONE\n",
    "        )\n",
    "\n",
    "        if len(contours) > 0:\n",
    "            main_contour = max(contours, key=cv2.contourArea)\n",
    "\n",
    "            # Calculate distances from boundary to centroid\n",
    "            M = cv2.moments(main_contour)\n",
    "            if M['m00'] != 0:\n",
    "                cx = int(M['m10']/M['m00'])\n",
    "                cy = int(M['m01']/M['m00'])\n",
    "\n",
    "                distances = []\n",
    "                for point in main_contour:\n",
    "                    x, y = point[0]\n",
    "                    dist = np.sqrt((x - cx)**2 + (y - cy)**2)\n",
    "                    distances.append(dist)\n",
    "\n",
    "                features['boundary_roughness'] = np.std(distances) if distances else 0\n",
    "            else:\n",
    "                features['boundary_roughness'] = 0\n",
    "\n",
    "            # Convexity defects\n",
    "            hull = cv2.convexHull(main_contour, returnPoints=False)\n",
    "            if len(hull) > 2:\n",
    "                defects = cv2.convexityDefects(main_contour, hull)\n",
    "                features['convexity_defects'] = len(defects) if defects is not None else 0\n",
    "            else:\n",
    "                features['convexity_defects'] = 0\n",
    "\n",
    "        # Overall shape complexity (combination of features)\n",
    "        features['shape_complexity'] = (\n",
    "            features['fractal_dimension'] +\n",
    "            features['boundary_roughness'] * 0.01 +\n",
    "            features['convexity_defects'] * 0.1\n",
    "        )\n",
    "\n",
    "        return features\n",
    "\n",
    "    def _calculate_fractal_dimension(self, shape_mask, max_box_size=32):\n",
    "        \"\"\"Calculate fractal dimension using box counting method\"\"\"\n",
    "        if shape_mask.sum() == 0:\n",
    "            return 0.0\n",
    "\n",
    "        # Find boundary pixels\n",
    "        contours, _ = cv2.findContours(\n",
    "            (shape_mask * 255).astype(np.uint8),\n",
    "            cv2.RETR_EXTERNAL,\n",
    "            cv2.CHAIN_APPROX_NONE\n",
    "        )\n",
    "\n",
    "        if len(contours) == 0:\n",
    "            return 0.0\n",
    "\n",
    "        main_contour = max(contours, key=cv2.contourArea)\n",
    "        boundary_pixels = main_contour.reshape(-1, 2)\n",
    "\n",
    "        if len(boundary_pixels) < 10:\n",
    "            return 1.0  # Minimum complexity\n",
    "\n",
    "        # Box counting\n",
    "        box_sizes = []\n",
    "        counts = []\n",
    "\n",
    "        for box_size in range(2, min(max_box_size, len(boundary_pixels)//4)):\n",
    "            # Create grid\n",
    "            min_coords = boundary_pixels.min(axis=0)\n",
    "            max_coords = boundary_pixels.max(axis=0)\n",
    "            grid_size = np.ceil((max_coords - min_coords) / box_size).astype(int)\n",
    "\n",
    "            # Count occupied boxes\n",
    "            occupied = set()\n",
    "            for pixel in boundary_pixels:\n",
    "                grid_pos = ((pixel - min_coords) / box_size).astype(int)\n",
    "                occupied.add(tuple(grid_pos))\n",
    "\n",
    "            if len(occupied) > 0:\n",
    "                box_sizes.append(box_size)\n",
    "                counts.append(len(occupied))\n",
    "\n",
    "        # Calculate fractal dimension\n",
    "        if len(box_sizes) >= 3:\n",
    "            # Linear regression on log-log plot\n",
    "            log_sizes = np.log(1.0 / np.array(box_sizes))\n",
    "            log_counts = np.log(np.array(counts))\n",
    "\n",
    "            # Simple linear regression\n",
    "            slope, intercept = np.polyfit(log_sizes, log_counts, 1)\n",
    "            fractal_dim = slope\n",
    "        else:\n",
    "            fractal_dim = 1.0\n",
    "\n",
    "        return max(1.0, min(2.0, fractal_dim))  # Clamp to reasonable range\n",
    "\n",
    "    # Texture features\n",
    "    def extract_texture_features(self, fingerprint):\n",
    "        \"\"\"Extract texture features using GLCM from distance transform\"\"\"\n",
    "        distance_transform = fingerprint[:, :, 1]\n",
    "\n",
    "        # Normalize to 0-255 range for GLCM\n",
    "        if distance_transform.max() > distance_transform.min():\n",
    "            texture_img = ((distance_transform - distance_transform.min()) /\n",
    "                          (distance_transform.max() - distance_transform.min()) * 255).astype(np.uint8)\n",
    "        else:\n",
    "            texture_img = (distance_transform * 255).astype(np.uint8)\n",
    "\n",
    "        features = {}\n",
    "\n",
    "        try:\n",
    "            # Calculate GLCM (Gray Level Co-occurrence Matrix)\n",
    "            glcm = graycomatrix(texture_img, distances=[1], angles=[0, np.pi/4, np.pi/2, 3*np.pi/4],\n",
    "                               symmetric=True, normed=True)\n",
    "\n",
    "            # Extract texture features\n",
    "            features['texture_contrast'] = graycoprops(glcm, 'contrast').mean()\n",
    "            features['texture_homogeneity'] = graycoprops(glcm, 'homogeneity').mean()\n",
    "            features['texture_energy'] = graycoprops(glcm, 'energy').mean()\n",
    "            features['texture_correlation'] = graycoprops(glcm, 'correlation').mean()\n",
    "\n",
    "        except Exception as e:\n",
    "            # Fallback values if GLCM calculation fails\n",
    "            features['texture_contrast'] = 0\n",
    "            features['texture_homogeneity'] = 0\n",
    "            features['texture_energy'] = 0\n",
    "            features['texture_correlation'] = 0\n",
    "\n",
    "        return features\n",
    "\n",
    "    # Curvature features\n",
    "    def extract_curvature_features(self, fingerprint):\n",
    "        \"\"\"Extract curvature-based features from boundary curvature channel\"\"\"\n",
    "        curvature_map = fingerprint[:, :, 2]\n",
    "\n",
    "        features = {}\n",
    "\n",
    "        # Basic curvature statistics\n",
    "        features['mean_curvature'] = np.mean(curvature_map)\n",
    "        features['curvature_variance'] = np.var(curvature_map)\n",
    "        features['max_curvature'] = np.max(curvature_map)\n",
    "\n",
    "        # Count curvature peaks (local maxima)\n",
    "        # Apply Gaussian blur to reduce noise\n",
    "        smoothed = cv2.GaussianBlur(curvature_map.astype(np.float32), (5, 5), 1.0)\n",
    "\n",
    "        # Find local maxima\n",
    "        kernel = np.array([[-1,-1,-1],\n",
    "                          [-1, 8,-1],\n",
    "                          [-1,-1,-1]])\n",
    "        peaks = cv2.filter2D(smoothed, -1, kernel)\n",
    "        threshold = np.mean(smoothed) + np.std(smoothed)\n",
    "        features['curvature_peaks'] = np.sum(peaks > threshold)\n",
    "\n",
    "        return features\n",
    "\n",
    "    # Multi-scale features\n",
    "    def extract_multiscale_features(self, fingerprint, scales=[0.25, 0.5, 1.0, 2.0]):\n",
    "        \"\"\"Extract features at multiple scales\"\"\"\n",
    "        features = {}\n",
    "\n",
    "        areas = []\n",
    "        perimeters = []\n",
    "        complexities = []\n",
    "\n",
    "        for scale in scales:\n",
    "            # Resize fingerprint\n",
    "            h, w = fingerprint.shape[:2]\n",
    "            new_size = (int(w * scale), int(h * scale))\n",
    "\n",
    "            if new_size[0] > 0 and new_size[1] > 0:\n",
    "                scaled = cv2.resize(fingerprint, new_size, interpolation=cv2.INTER_LINEAR)\n",
    "\n",
    "                # Extract shape features at this scale\n",
    "                scale_features = self.extract_shape_features(scaled)\n",
    "                scale_complexity = self.extract_complexity_features(scaled)\n",
    "\n",
    "                areas.append(scale_features.get('area', 0))\n",
    "                perimeters.append(scale_features.get('perimeter', 0))\n",
    "                complexities.append(scale_complexity.get('shape_complexity', 0))\n",
    "\n",
    "        # Store multi-scale statistics\n",
    "        features['multi_scale_area'] = np.mean(areas) if areas else 0\n",
    "        features['multi_scale_perimeter'] = np.mean(perimeters) if perimeters else 0\n",
    "        features['multi_scale_complexity'] = np.mean(complexities) if complexities else 0\n",
    "\n",
    "        return features\n",
    "\n",
    "    # Main feature extraction method\n",
    "    def extract_all_features(self, fingerprint):\n",
    "        \"\"\"Extract all features from a single fingerprint\"\"\"\n",
    "        if fingerprint is None or fingerprint.shape[-1] != 4:\n",
    "            return {name: 0 for name in self.feature_names}\n",
    "\n",
    "        features = {}\n",
    "\n",
    "        try:\n",
    "            # Extract each feature group\n",
    "            features.update(self.extract_shape_features(fingerprint))\n",
    "            features.update(self.extract_complexity_features(fingerprint))\n",
    "            features.update(self.extract_texture_features(fingerprint))\n",
    "            features.update(self.extract_curvature_features(fingerprint))\n",
    "            features.update(self.extract_multiscale_features(fingerprint))\n",
    "\n",
    "            # Ensure all features are present\n",
    "            for name in self.feature_names:\n",
    "                if name not in features:\n",
    "                    features[name] = 0\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error extracting features: {e}\")\n",
    "            features = {name: 0 for name in self.feature_names}\n",
    "\n",
    "        return features\n",
    "\n",
    "    def batch_extract_features(self, fingerprints, show_progress=True):\n",
    "        \"\"\"Extract features from multiple fingerprints\"\"\"\n",
    "        from tqdm import tqdm\n",
    "\n",
    "        all_features = []\n",
    "        iterator = tqdm(fingerprints) if show_progress else fingerprints\n",
    "\n",
    "        for fingerprint in iterator:\n",
    "            features = self.extract_all_features(fingerprint)\n",
    "            all_features.append(features)\n",
    "\n",
    "        # Convert to DataFrame\n",
    "        features_df = pd.DataFrame(all_features)\n",
    "        features_df = features_df[self.feature_names]  # Ensure consistent column order\n",
    "\n",
    "        return features_df\n",
    "\n",
    "    def normalize_features(self, features_df):\n",
    "        \"\"\"Normalize features using robust statistics\"\"\"\n",
    "        normalized_df = features_df.copy()\n",
    "\n",
    "        for column in normalized_df.columns:\n",
    "            data = normalized_df[column].values\n",
    "\n",
    "            # Use robust statistics (median and IQR)\n",
    "            median = np.median(data)\n",
    "            q75, q25 = np.percentile(data, [75, 25])\n",
    "            iqr = q75 - q25\n",
    "\n",
    "            if iqr > 0:\n",
    "                # Robust z-score normalization\n",
    "                normalized_df[column] = (data - median) / iqr\n",
    "            else:\n",
    "                # If no variation, set to 0\n",
    "                normalized_df[column] = 0\n",
    "\n",
    "        return normalized_df\n",
    "\n",
    "    def get_feature_importance(self, features_df, target_labels, task='fire_type'):\n",
    "        \"\"\"Analyze feature importance for classification tasks\"\"\"\n",
    "        from sklearn.ensemble import RandomForestClassifier\n",
    "        from sklearn.model_selection import cross_val_score\n",
    "\n",
    "        # Prepare target\n",
    "        y = np.array([label[task] for label in target_labels])\n",
    "\n",
    "        # Remove constant features\n",
    "        feature_cols = features_df.select_dtypes(include=[np.number]).columns\n",
    "        features_clean = features_df[feature_cols].fillna(0)\n",
    "\n",
    "        # Train random forest for feature importance\n",
    "        rf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "        rf.fit(features_clean, y)\n",
    "\n",
    "        # Get feature importances\n",
    "        importances = rf.feature_importances_\n",
    "        feature_names = features_clean.columns\n",
    "\n",
    "        # Sort by importance\n",
    "        indices = np.argsort(importances)[::-1]\n",
    "\n",
    "        return {\n",
    "            'feature_names': feature_names[indices].tolist(),\n",
    "            'importances': importances[indices].tolist(),\n",
    "            'classifier_score': rf.score(features_clean, y)\n",
    "        }\n",
    "\n",
    "print(\"âœ“ Complete Fire Pattern Analyzer class created\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55423489",
   "metadata": {},
   "source": [
    "## ðŸ§ª Feature Extraction Demonstration\n",
    "#\n",
    "Let's test our feature extraction system with sample fingerprints.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cf3a93b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize analyzer\n",
    "analyzer = FirePatternAnalyzer()\n",
    "\n",
    "print(f\"Feature analyzer initialized with {len(analyzer.feature_names)} features:\")\n",
    "for name, desc in list(analyzer.feature_descriptions.items())[:10]:  # Show first 10\n",
    "    print(f\"  {name}: {desc}\")\n",
    "if len(analyzer.feature_descriptions) > 10:\n",
    "    print(f\"  ... and {len(analyzer.feature_descriptions) - 10} more features\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb28c6ca",
   "metadata": {},
   "source": [
    "## ðŸ”¬ Single Fingerprint Analysis\n",
    "#\n",
    "Let's analyze a single fingerprint to understand what features reveal about fire patterns.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d58a6304",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load sample data\n",
    "fingerprints, labels, metadata, encoders = load_processed_data(\"demo_processed_data\")\n",
    "\n",
    "# Extract features from first fingerprint\n",
    "sample_fingerprint = fingerprints[0]\n",
    "sample_features = analyzer.extract_all_features(sample_fingerprint)\n",
    "\n",
    "print(\"ðŸŽ¯ Single Fingerprint Feature Analysis\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"Sample fire: {metadata[0]['fire_id']}\")\n",
    "print(f\"Fire type: {metadata[0]['original_fire_type']}\")\n",
    "print(f\"Area: {metadata[0]['area_ha']:.1f} ha\")\n",
    "\n",
    "print(f\"\\nExtracted {len(sample_features)} features:\")\n",
    "print(\"\\nShape Features:\")\n",
    "shape_features = ['area', 'perimeter', 'compactness', 'elongation', 'solidity', 'extent', 'eccentricity', 'orientation']\n",
    "for feat in shape_features:\n",
    "    if feat in sample_features:\n",
    "        print(f\"  {feat}: {sample_features[feat]:.3f}\")\n",
    "\n",
    "print(\"\\nComplexity Features:\")\n",
    "complexity_features = ['fractal_dimension', 'boundary_roughness', 'convexity_defects', 'shape_complexity']\n",
    "for feat in complexity_features:\n",
    "    if feat in sample_features:\n",
    "        print(f\"  {feat}: {sample_features[feat]:.3f}\")\n",
    "\n",
    "print(\"\\nTexture Features:\")\n",
    "texture_features = ['texture_contrast', 'texture_homogeneity', 'texture_energy', 'texture_correlation']\n",
    "for feat in texture_features:\n",
    "    if feat in sample_features:\n",
    "        print(f\"  {feat}: {sample_features[feat]:.3f}\")\n",
    "\n",
    "print(\"\\nCurvature Features:\")\n",
    "curvature_features = ['mean_curvature', 'curvature_variance', 'max_curvature', 'curvature_peaks']\n",
    "for feat in curvature_features:\n",
    "    if feat in sample_features:\n",
    "        print(f\"  {feat}: {sample_features[feat]:.3f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fe85fd9",
   "metadata": {},
   "source": [
    "## ðŸ“Š Batch Feature Extraction\n",
    "#\n",
    "Process all fingerprints in our dataset to create a comprehensive feature matrix.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ecfdc31",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract features from all fingerprints\n",
    "print(\"ðŸ”„ Batch feature extraction from all fingerprints...\")\n",
    "features_df = analyzer.batch_extract_features(fingerprints, show_progress=True)\n",
    "\n",
    "print(f\"âœ“ Extracted features for {len(features_df)} fingerprints\")\n",
    "print(f\"Feature matrix shape: {features_df.shape}\")\n",
    "\n",
    "# Normalize features\n",
    "normalized_features = analyzer.normalize_features(features_df)\n",
    "\n",
    "print(\"âœ“ Features normalized using robust statistics\")\n",
    "\n",
    "# Save features\n",
    "features_df.to_csv('demo_fire_features.csv', index=False)\n",
    "normalized_features.to_csv('demo_fire_features_normalized.csv', index=False)\n",
    "\n",
    "print(\"âœ“ Features saved to CSV files\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "246bde4b",
   "metadata": {},
   "source": [
    "## ðŸ“ˆ Feature Distribution Analysis\n",
    "#\n",
    "Visualize the distributions of our extracted features to understand fire pattern characteristics.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a83ee0d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_feature_distributions(features_df, normalized_df):\n",
    "    \"\"\"Analyze and visualize feature distributions\"\"\"\n",
    "    fig, axes = plt.subplots(3, 3, figsize=(18, 15))\n",
    "\n",
    "    # Select representative features from each category\n",
    "    plot_features = [\n",
    "        'area', 'compactness', 'elongation',  # Shape\n",
    "        'fractal_dimension', 'boundary_roughness', 'shape_complexity',  # Complexity\n",
    "        'texture_contrast', 'mean_curvature', 'curvature_peaks'  # Texture & Curvature\n",
    "    ]\n",
    "\n",
    "    for i, feature in enumerate(plot_features):\n",
    "        row, col = divmod(i, 3)\n",
    "\n",
    "        # Plot normalized features\n",
    "        data = normalized_df[feature].dropna()\n",
    "        if len(data) > 0:\n",
    "            axes[row, col].hist(data, bins=30, alpha=0.7, density=True)\n",
    "            axes[row, col].axvline(data.mean(), color='red', linestyle='--', label='Mean')\n",
    "            axes[row, col].axvline(data.median(), color='green', linestyle='--', label='Median')\n",
    "            axes[row, col].set_title(f'{feature.replace(\"_\", \" \").title()}')\n",
    "            axes[row, col].set_xlabel('Normalized Value')\n",
    "            axes[row, col].set_ylabel('Density')\n",
    "            axes[row, col].legend()\n",
    "            axes[row, col].grid(True, alpha=0.3)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('feature_distributions.png', dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "\n",
    "    # Feature statistics summary\n",
    "    print(\"ðŸ“Š Feature Statistics Summary\")\n",
    "    print(\"=\" * 40)\n",
    "    stats_summary = normalized_df.describe().T[['mean', 'std', 'min', 'max', '50%']]\n",
    "    stats_summary.columns = ['Mean', 'Std', 'Min', 'Max', 'Median']\n",
    "\n",
    "    print(stats_summary.round(3))\n",
    "\n",
    "# Analyze feature distributions\n",
    "analyze_feature_distributions(features_df, normalized_features)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1beeb243",
   "metadata": {},
   "source": [
    "## ðŸ”— Feature Correlation Analysis\n",
    "#\n",
    "Understand how different features relate to each other and what they reveal about fire patterns.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03b0cef0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_feature_correlations(features_df, top_n=15):\n",
    "    \"\"\"Analyze correlations between features\"\"\"\n",
    "    # Select numeric features\n",
    "    numeric_features = features_df.select_dtypes(include=[np.number]).columns\n",
    "    feature_data = features_df[numeric_features].fillna(0)\n",
    "\n",
    "    # Calculate correlation matrix\n",
    "    corr_matrix = feature_data.corr()\n",
    "\n",
    "    # Find most correlated feature pairs\n",
    "    corr_pairs = []\n",
    "    for i in range(len(corr_matrix.columns)):\n",
    "        for j in range(i+1, len(corr_matrix.columns)):\n",
    "            corr_pairs.append((\n",
    "                corr_matrix.columns[i],\n",
    "                corr_matrix.columns[j],\n",
    "                corr_matrix.iloc[i, j]\n",
    "            ))\n",
    "\n",
    "    # Sort by absolute correlation\n",
    "    corr_pairs.sort(key=lambda x: abs(x[2]), reverse=True)\n",
    "\n",
    "    print(\"ðŸ”— Top Feature Correlations\")\n",
    "    print(\"=\" * 40)\n",
    "    for feat1, feat2, corr in corr_pairs[:top_n]:\n",
    "        print(f\"{feat1} â†” {feat2}: {corr:.3f}\")\n",
    "\n",
    "    # Visualize correlation heatmap\n",
    "    plt.figure(figsize=(12, 10))\n",
    "\n",
    "    # Select subset of features for clearer visualization\n",
    "    subset_features = [\n",
    "        'area', 'perimeter', 'compactness', 'elongation', 'solidity',\n",
    "        'fractal_dimension', 'boundary_roughness', 'shape_complexity',\n",
    "        'texture_contrast', 'texture_energy', 'mean_curvature', 'curvature_variance'\n",
    "    ]\n",
    "\n",
    "    subset_corr = corr_matrix.loc[subset_features, subset_features]\n",
    "\n",
    "    sns.heatmap(subset_corr, annot=True, cmap='coolwarm', center=0,\n",
    "                fmt='.2f', square=True, cbar_kws={'shrink': 0.8})\n",
    "    plt.title('Feature Correlation Heatmap')\n",
    "    plt.xticks(rotation=45, ha='right')\n",
    "    plt.yticks(rotation=0)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('feature_correlations.png', dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "\n",
    "    return corr_matrix\n",
    "\n",
    "# Analyze feature correlations\n",
    "correlation_matrix = analyze_feature_correlations(features_df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c2afce4",
   "metadata": {},
   "source": [
    "## ðŸŽ¯ Feature Importance Analysis\n",
    "#\n",
    "Determine which features are most important for classifying different fire characteristics.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7818f480",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_feature_importance(features_df, labels, tasks=['fire_type', 'size_category']):\n",
    "    \"\"\"Analyze feature importance for different classification tasks\"\"\"\n",
    "    fig, axes = plt.subplots(len(tasks), 1, figsize=(12, 6*len(tasks)))\n",
    "\n",
    "    if len(tasks) == 1:\n",
    "        axes = [axes]\n",
    "\n",
    "    for i, task in enumerate(tasks):\n",
    "        print(f\"\\nðŸŽ¯ Feature Importance for {task.replace('_', ' ').title()}\")\n",
    "        print(\"-\" * 50)\n",
    "\n",
    "        # Get feature importance\n",
    "        importance_results = analyzer.get_feature_importance(features_df, labels, task)\n",
    "\n",
    "        print(f\"Classifier accuracy: {importance_results['classifier_score']:.3f}\")\n",
    "\n",
    "        # Plot top 10 features\n",
    "        top_n = 10\n",
    "        feature_names = importance_results['feature_names'][:top_n]\n",
    "        importances = importance_results['importances'][:top_n]\n",
    "\n",
    "        axes[i].barh(range(len(feature_names)), importances)\n",
    "        axes[i].set_yticks(range(len(feature_names)))\n",
    "        axes[i].set_yticklabels([name.replace('_', ' ').title() for name in feature_names])\n",
    "        axes[i].set_xlabel('Importance Score')\n",
    "        axes[i].set_title(f'Top {top_n} Features for {task.replace(\"_\", \" \").title()} Classification')\n",
    "        axes[i].grid(True, alpha=0.3)\n",
    "\n",
    "        # Print top features\n",
    "        print(\"Top 10 features:\")\n",
    "        for j, (name, imp) in enumerate(zip(feature_names, importances)):\n",
    "            print(f\"  {j+1}. {name}: {imp:.4f}\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('feature_importance_analysis.png', dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "\n",
    "# Analyze feature importance\n",
    "analyze_feature_importance(normalized_features, labels)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de6b50e1",
   "metadata": {},
   "source": [
    "## ðŸ”¬ Feature-Fingerprint Relationship Visualization\n",
    "#\n",
    "Show how features relate to the visual appearance of fingerprints.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38124168",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_feature_fingerprint_relationship(fingerprints, features_df, feature_name, n_samples=6):\n",
    "    \"\"\"Visualize relationship between features and fingerprint appearance\"\"\"\n",
    "    # Get feature values\n",
    "    feature_values = features_df[feature_name].values\n",
    "\n",
    "    # Select samples with diverse feature values\n",
    "    sorted_indices = np.argsort(feature_values)\n",
    "    sample_indices = np.linspace(0, len(sorted_indices)-1, n_samples, dtype=int)\n",
    "    selected_indices = sorted_indices[sample_indices]\n",
    "\n",
    "    fig, axes = plt.subplots(n_samples, 5, figsize=(20, 4*n_samples))\n",
    "\n",
    "    channel_names = ['Shape', 'Distance', 'Curvature', 'Fractal', 'RGB Composite']\n",
    "\n",
    "    for i, idx in enumerate(selected_indices):\n",
    "        fingerprint = fingerprints[idx]\n",
    "        feature_value = feature_values[idx]\n",
    "        meta = metadata[idx]\n",
    "\n",
    "        # Plot each channel\n",
    "        for j in range(4):\n",
    "            axes[i, j].imshow(fingerprint[:, :, j], cmap='viridis')\n",
    "            axes[i, j].set_title(f'{channel_names[j]}')\n",
    "            axes[i, j].axis('off')\n",
    "\n",
    "        # RGB composite\n",
    "        rgb_composite = fingerprint[:, :, :3]\n",
    "        axes[i, 4].imshow(rgb_composite)\n",
    "        axes[i, 4].set_title('RGB Composite')\n",
    "        axes[i, 4].axis('off')\n",
    "\n",
    "        # Add feature value and fire info\n",
    "        feature_info = f\"{feature_name.replace('_', ' ').title()}: {feature_value:.3f}\\n\"\n",
    "        feature_info += f\"Fire {meta['fire_id']} - {meta['original_fire_type']}\"\n",
    "\n",
    "        axes[i, 0].text(-0.3, 0.5, feature_info, transform=axes[i, 0].transAxes,\n",
    "                       verticalalignment='center', fontsize=9,\n",
    "                       bbox=dict(boxstyle=\"round,pad=0.3\", facecolor=\"lightblue\"))\n",
    "\n",
    "    plt.suptitle(f'Fingerprint Variations by {feature_name.replace(\"_\", \" \").title()}',\n",
    "                 fontsize=16, y=0.98)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f'feature_fingerprint_{feature_name}.png', dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "\n",
    "# Visualize relationships for key features\n",
    "key_features = ['area', 'fractal_dimension', 'texture_contrast', 'mean_curvature']\n",
    "for feature in key_features:\n",
    "    if feature in features_df.columns:\n",
    "        visualize_feature_fingerprint_relationship(fingerprints, features_df, feature, n_samples=4)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b32df7c8",
   "metadata": {},
   "source": [
    "## ðŸ’¾ Feature Database Creation\n",
    "#\n",
    "Create a comprehensive feature database that can be used for similarity search and analysis.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0a715bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_feature_database(features_df, normalized_features, metadata, labels, output_path=\"fire_feature_database\"):\n",
    "    \"\"\"Create comprehensive feature database\"\"\"\n",
    "    output_dir = Path(output_path)\n",
    "    output_dir.mkdir(exist_ok=True)\n",
    "\n",
    "    print(f\"Creating feature database in {output_dir}...\")\n",
    "\n",
    "    # Save raw features\n",
    "    features_df.to_csv(output_dir / 'raw_features.csv', index=False)\n",
    "\n",
    "    # Save normalized features\n",
    "    normalized_features.to_csv(output_dir / 'normalized_features.csv', index=False)\n",
    "\n",
    "    # Save feature metadata\n",
    "    feature_metadata = {\n",
    "        'num_fires': len(features_df),\n",
    "        'num_features': len(features_df.columns),\n",
    "        'feature_names': features_df.columns.tolist(),\n",
    "        'feature_descriptions': analyzer.feature_descriptions,\n",
    "        'creation_date': str(pd.Timestamp.now()),\n",
    "        'normalization_method': 'robust_statistics'\n",
    "    }\n",
    "\n",
    "    with open(output_dir / 'feature_metadata.json', 'w') as f:\n",
    "        json.dump(feature_metadata, f, indent=2, default=str)\n",
    "\n",
    "    # Save fire metadata and labels\n",
    "    fire_info = []\n",
    "    for i, meta in enumerate(metadata):\n",
    "        fire_data = meta.copy()\n",
    "        fire_data.update(labels[i])\n",
    "        fire_info.append(fire_data)\n",
    "\n",
    "    fire_df = pd.DataFrame(fire_info)\n",
    "    fire_df.to_csv(output_dir / 'fire_metadata.csv', index=False)\n",
    "\n",
    "    # Create feature statistics summary\n",
    "    stats = {\n",
    "        'raw_stats': features_df.describe().to_dict(),\n",
    "        'normalized_stats': normalized_features.describe().to_dict(),\n",
    "        'feature_correlations': correlation_matrix.to_dict()\n",
    "    }\n",
    "\n",
    "    with open(output_dir / 'feature_statistics.json', 'w') as f:\n",
    "        json.dump(stats, f, indent=2, default=str)\n",
    "\n",
    "    print(\"âœ“ Feature database created with files:\")\n",
    "    print(f\"  - raw_features.csv: {len(features_df)} fires Ã— {len(features_df.columns)} features\")\n",
    "    print(f\"  - normalized_features.csv: Normalized feature matrix\")\n",
    "    print(f\"  - feature_metadata.json: Feature descriptions and metadata\")\n",
    "    print(f\"  - fire_metadata.csv: Fire information and labels\")\n",
    "    print(f\"  - feature_statistics.json: Statistical summaries\")\n",
    "\n",
    "    return str(output_dir)\n",
    "\n",
    "# Create comprehensive feature database\n",
    "database_path = create_feature_database(features_df, normalized_features, metadata, labels)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab04d35f",
   "metadata": {},
   "source": [
    "## ðŸŽ¯ Key Insights and Applications\n",
    "#\n",
    "### What We've Accomplished:\n",
    "#\n",
    "1. **Comprehensive Feature Extraction**: 20+ geometric and textural features\n",
    "2. **Multi-Scale Analysis**: Features computed at different scales\n",
    "3. **Advanced Pattern Recognition**: Fractal dimensions, curvature, texture analysis\n",
    "4. **Statistical Analysis**: Distributions, correlations, and importance analysis\n",
    "5. **Feature Database**: Structured database for similarity search applications\n",
    "#\n",
    "### Feature Categories and Insights:\n",
    "#\n",
    "#### Shape Features:\n",
    "- **Area & Perimeter**: Basic fire size measurements\n",
    "- **Compactness**: How \"circular\" the fire is (higher = more contained)\n",
    "- **Elongation**: Fire spread pattern (higher = more elongated)\n",
    "- **Solidity**: Boundary complexity (lower = more irregular)\n",
    "#\n",
    "#### Complexity Features:\n",
    "- **Fractal Dimension**: Self-similarity and natural complexity (1.0-2.0 range)\n",
    "- **Boundary Roughness**: Edge irregularity and burning patterns\n",
    "- **Convexity Defects**: Number of \"bays\" or indentations in fire boundary\n",
    "#\n",
    "#### Texture Features:\n",
    "- **Contrast**: Variation in distance transform (spread complexity)\n",
    "- **Homogeneity**: Spatial uniformity of fire growth\n",
    "- **Energy**: Overall texture strength\n",
    "#\n",
    "#### Curvature Features:\n",
    "- **Mean/Max Curvature**: Boundary bending and complexity\n",
    "- **Curvature Peaks**: Number of major boundary changes\n",
    "#\n",
    "### Applications:\n",
    "#\n",
    "1. **Fire Classification**: Features enable accurate fire type prediction\n",
    "2. **Similarity Search**: Find fires with similar burning patterns\n",
    "3. **Risk Assessment**: Complex fires may indicate challenging terrain\n",
    "4. **Pattern Discovery**: Clustering reveals common fire archetypes\n",
    "5. **Research**: Quantitative analysis of fire behavior patterns\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c8e6bb8",
   "metadata": {},
   "source": [
    "## ðŸš€ Summary\n",
    "#\n",
    "**Congratulations!** You've built a comprehensive fire pattern analysis system:\n",
    "#\n",
    "- âœ… **20+ geometric features** extracted from 4-channel fingerprints\n",
    "- âœ… **Advanced complexity analysis** including fractal dimensions\n",
    "- âœ… **Texture and curvature analysis** revealing burning patterns\n",
    "- âœ… **Statistical insights** into feature distributions and correlations\n",
    "- âœ… **Feature importance analysis** for classification tasks\n",
    "- âœ… **Structured feature database** ready for similarity search\n",
    "#\n",
    "**Next notebook**: We'll use these features to build a powerful similarity search\n",
    "engine that can find fires with similar patterns and characteristics.\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"ðŸŽ‰ PATTERN ANALYSIS & FEATURE EXTRACTION COMPLETE!\")\n",
    "print(\"=\"*60)\n",
    "print(\"Ready for the next phase: Similarity Search & Clustering\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
